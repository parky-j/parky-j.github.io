---
layout: default
---

{%- if site.posts.size > 0 -%}
  <ul class="posts">
    <li class="posts-labelgroup" id="posts-labelgroup">
      <h1 id="posts-label">posts</h1>
      {% if site.plainwhite.search %}
      <div class="search-container">
        <div class="search-section">
          <i class="icon-search"></i>
          <input type="text" name="search" id="searchbar" autocomplete="off" aria-label="search in posts">
        </div>
        <div class="search-results" id="search-results" data-placeholder="No Results" style="display: none;"></div>
      </div>
      {% endif %}
    </li>
  </ul>
{%- endif -%}

<ul class="posts">
  <div class="post">
    <body><article id="83817704-3c47-4405-bcbc-09826a13153b" class="page sans"><header><p class="page-description"></p></header><div class="page-body"><p id="2750c0c7-0478-4f5f-8b45-28524c14b363" class="block-color-gray">
    </p>Update:<i class="icon-calendar">2024.02.01</i><h1 id="37b57799-74c7-437f-bd5b-15dce854a8ef" class="">Welcome!</h1><p id="8ce6a98a-50ad-4a10-8e60-86dd9cb0bdd2" class="block-color-gray">I work on overcoming instability and noisy data of Affective computing in the wild setting. Throughout my research, I have studied various kinds of topics to help AI deal with real-world problems in emotion recognition. Specifically, I am interested in advancing AI to better inference facial expression recognition and physiological signal sensing, such as in remote photoplethysmography and breathing rate.</p>
    <div id="596c216b-4ee1-4d7c-8853-03ef2daaf757" class="column-list"><div id="c62f33d1-5810-48bf-98a3-5ad2b968ccd0" style="width:30%" class="column"><h2 id="f4934c89-20dd-41e8-b503-1424926cb2ed" class="">Interests</h2><ul id="d38ae99c-e983-41c0-8a25-54e8546d46b7" class="block-color-gray bulleted-list"><br/><li style="list-style-type:disc">Computer Vision</li><br/></ul><ul id="1d15fcab-5da0-41f4-ab11-f0027f8f48b2" class="block-color-gray bulleted-list"><li style="list-style-type:disc">Affective Computing</li><br/></ul><ul id="13590a4a-c86a-4608-84bc-5f98b90a6e26" class="block-color-gray bulleted-list"><li style="list-style-type:disc">Remote Photoplethysmography</li><br/></ul><ul id="d9bb3f00-8714-4927-ae78-a3304623cb45" class="block-color-gray bulleted-list"><li style="list-style-type:disc">Facial Expression Recognition</li><br/></ul><p id="ac6ab240-e2a5-4d07-91c8-ec90ae47178c" class="">
</p></div>
    <div id="0b5b8250-cc2f-4fd2-b81e-2950c7ee86e3" style="width:65%" class="column"><h2 id="a1005f15-83d9-4481-ae1a-c862279a0a0e" class="">Education</h2><li style="list-style-type:disc">Ph.D., in Graduate School of Computer Science, Sangmyung University, Seoul, Korea (Mar 2019 – Feb 2024) <br/>Thesis: Camera-based Remote Photoplethysmography and Heart Rate Variability Measurement for Real-time Driver Monitoring<br/>Adviser:<a href="https://pr.smu.ac.kr/people/professor">Eui Chul Lee</a></li><li style="list-style-type:disc">M.S., in Graduate School of Computer Science, Sangmyung University, Seoul, Korea (Mar 2017 – Feb 2019)<br/>Thesis: A Study on temporal-spatial feature based machine learning model for spontaneous/posed smile facial expression classification<br/>Adviser:<a href="https://pr.smu.ac.kr/people/professor">Eui Chul Lee</a><//li></div></div>
      <hr id="cb3f9816-a972-4e00-9ad1-7ef0c5fa9359"/><h1 id="a65d9784-a50b-4f27-b7aa-38c735c538c3" class="">Publications [<a href="https://scholar.google.co.kr/citations?user=lU9NpJUAAAAJ&amp;hl=en">Google Scholar Profile</a>]</h1><p id="0f8e0098-ce7a-46f8-9667-caea1a3c4301" class=""><strong>[C#]: Conference, [J#]: Journal, [W#]: workshops (#: Count)</strong></p><h3 id="e0296776-1f89-45c3-b917-7f05ca8e29f7" class="">2023</h3><blockquote id="20efdfe3-0877-4dbf-a50b-80c0bbe8d9e6" class=""><strong>[J11]</strong> <strong>Kunyoung Lee</strong>, Jaemu Oh, Hojoon You, and Eui Chul Lee. <strong>Improving Remote Photoplethysmography Performance through Deep-Learning-Based Real-Time Skin Segmentation Network.</strong> <em>Electronics</em> 12, no. 17 (2023): 3729.</blockquote><blockquote id="6dc864c9-bfcc-4921-a9d0-5747d2839d52" class=""><strong>[J10]</strong> <strong>Kunyoung Lee</strong>, Seunghyun Kim, Byeongseon An, Hyunsoo Seo, Shinwi Park, and Eui Chul Lee. <strong>Noise-Assessment-Based Screening Method for Remote Photoplethysmography Estimation.</strong> <em>Applied Sciences</em> 13, no. 17 (2023): 9818.</blockquote><blockquote id="e0354384-3f62-4bcb-95b1-3e16c21fe867" class=""><strong>[J9]</strong> Jin, Eunju, Hyunju Kang, <strong>Kunyoung Lee</strong>, Seung Gun Lee, and Eui Chul Lee. <strong>Analysis of Nursing Students’ Nonverbal Communication Patterns during Simulation Practice: A Pilot Study.</strong> In <em>Healthcare</em>, vol. 11, no. 16, p. 2335. MDPI, 2023.</blockquote><blockquote id="8edbf65d-e1fb-4a5e-9711-beaeeb34453a" class=""><strong>[J8]</strong> <strong>Kunyoung Lee</strong>, Seunghyun Kim, and Eui Chul Lee. <strong>Fast and Accurate Facial Expression Image Classification and Regression Method Based on Knowledge Distillation.</strong> <em>Applied Sciences</em> 13, no. 11 (2023): 6409.</blockquote><blockquote id="c7e54347-e379-4ef5-af9d-352ace4dcc90" class=""><strong>[J7]</strong> You, Hojoon, <strong>Kunyoung Lee</strong>, Jaemu Oh, and Eui Chul Lee. <strong>Efficient and Low Color Information Dependency Skin Segmentation Model.</strong> <em>Mathematics</em> 11, no. 9 (2023): 2057.</blockquote><blockquote id="315dcba4-0eb8-4294-b3ff-9840315cf22c" class=""><strong>[W1]</strong> Kim, Seunghyun, <strong>Kunyoung Lee</strong>, and Eui Chul Lee. <strong>Multi-View Body Image-Based Prediction of Body Mass Index and Various Body Part Sizes.</strong> In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp. 6033-6040. 2023.</blockquote><h3 id="e14de6ab-ccdb-47e2-b798-33052a72a411" class="">2022</h3><blockquote id="759382ef-a770-48d7-9040-917c78415e80" class=""><strong>[J6]</strong> Hwang, Hyeonsang, <strong>Kunyoung Lee</strong>, and Eui Chul Lee. <strong>A real-time remote respiration measurement method with improved robustness based on a CNN model.</strong> <em>Applied Sciences</em> 12, no. 22 (2022): 11603.</blockquote><blockquote id="46170dbf-5306-47dd-be6a-25bb36296b4c" class=""><strong>[J5]</strong> Jang, Woohyuk, Chaewon Lee, Dae Sik Jeong, <strong>Kunyoung Lee</strong>, and Eui Chul Lee. <strong>Multi-Currency Integrated Serial Number Recognition Model of Images Acquired by Banknote Counters.</strong> <em>Sensors</em> 22, no. 22 (2022): 8612.</blockquote><blockquote id="d2a9abd2-5e89-4cce-8181-8fa2e22360ac" class=""><strong>[C3]</strong> <strong>Kunyoung Lee</strong>, Hojoon You, Jaemu Oh, and Eui Chul Lee. <strong>Extremely Lightweight Skin Segmentation Networks to Improve Remote Photoplethysmography Measurement.</strong> In <em>International Conference on Intelligent Human Computer Interaction</em>, pp. 454-459. Cham: Springer Nature Switzerland, 2022.</blockquote><h3 id="a75770e6-fa9b-40e2-b4b1-260c7221b092" class="">2021</h3><blockquote id="0bdf295e-9cb0-4dc0-b7e4-81f10764eded" class=""><strong>[C2]</strong> <strong>Kunyoung Lee</strong>, Kyungwon Jin, Youngwon Kim, Jee Hang Lee, and Eui Chul Lee. <strong>A comparative analysis on the impact of face tracker and skin segmentation onto improving the performance of real-time remote photoplethysmography.</strong> In <em>Intelligent Human Computer Interaction: 12th International Conference, IHCI 2020, Daegu, South Korea, November 24–26, 2020, Proceedings, Part II 12</em>, pp. 27-37. Springer International Publishing, 2021.</blockquote><h3 id="66557549-d03e-4ced-9f37-7c7bf54cdb78" class="">2020</h3><blockquote id="8f3bafe7-dff4-4074-af3d-5e330921434f" class=""><strong>[J4]</strong> <strong>Kunyoung Lee</strong>, and Eui Chul Lee. <strong>Siamese Architecture-Based 3D DenseNet with Person-Specific Normalization Using Neutral Expression for Spontaneous and Posed Smile Classification.</strong> <em>Sensors</em> 20, no. 24 (2020): 7184.</blockquote><blockquote id="91c6d21a-a576-408f-84b3-c3d4f0f8f43c" class=""><strong>[J3]</strong> Nam, Uiseo, <strong>Kunyoung Lee</strong>, Hyunwoong Ko, Jun-Young Lee, and Eui Chul Lee. <strong>Analyzing facial and eye movements to screen for Alzheimer’s disease.</strong> <em>Sensors</em> 20, no. 18 (2020): 5349.</blockquote><blockquote id="0b07681e-f164-4ea6-8281-6283ab3cafc5" class=""><strong>[J2]</strong> Park, Seho, <strong>Kunyoung Lee</strong> , Jae-A. Lim, Hyunwoong Ko, Taehoon Kim, Jung-In Lee, Hakrim Kim et al. <strong>Differences in facial expressions between spontaneous and posed smiles: Automated method by action units and three-dimensional facial landmarks.</strong> <em>Sensors</em> 20, no. 4 (2020): 1199.</blockquote><h3 id="eed6a270-afb2-46d4-96d2-f55207334fa0" class="">2019</h3><blockquote id="dd501c88-d17a-49fc-849f-af67b0261242" class=""><strong>[J1]</strong> <strong>Kunyoung Lee</strong>, and Eui Chul Lee. <strong>Comparison of facial expression recognition performance according to the use of depth information of structured-light type RGB-D camera.</strong> <em>Journal of Ambient Intelligence and Humanized Computing</em> (2019): 1-17.</blockquote><h3 id="a2a72277-a364-4408-b353-7db4d11b6dda" class="">2018</h3><blockquote id="49b41686-435c-4b73-abdb-c7b828ace851" class=""><strong>[C1]</strong> <strong>Kunyoung Lee</strong>, and Eui Chul Lee. <strong>Comparison of 2D&amp;3D Performances of Facial Feature Analysis Using RGB-D Vision Sensor.</strong> In <em>Advances in Computer Science and Ubiquitous Computing: CSA-CUTE 17</em>, pp. 1416-1421. Springer Singapore, 2018.</blockquote><h3 id="f9a5e59e-583a-441f-83e3-b0b72a41c42a" class="">Domestic Journals (4건)</h3><blockquote id="9eb80c95-ba75-403c-83e0-c5aca091c440" class="">황정원, <strong>이건영</strong>, 이의철, <strong>&quot;객체 추적 알고리즘 기반 비전 검사를 통한 압출부 타겟 검출 방법&quot;,</strong> Journal of Next-generation Convergence Technology Association, 7(9), pp. 1412~1420, 2023년 9월 (KCI)</blockquote><blockquote id="5ee4343d-300b-4abb-ae3f-8249e66c4420" class="">이채원, 윤성빈, 조철우, 황현상, <strong>이건영</strong>, 이의철, <strong>&quot;ESRGAN을 이용한 차량 번호판 화질 개선을 통한 인식률 향상,&quot;</strong> Journal of Next-generation Convergence Technology Association, 6(1), pp. 5-11, 2022년 1월.</blockquote><blockquote id="2c57197a-87c2-4740-a6e5-81d0f2b8f74c" class="">안병선, <strong>이건영</strong>, 이의철, <strong>&quot;표정 정보를 보존하는 선택적 얼굴 비식별화 방법,&quot;</strong> Journal of Next-generation Convergence Technology Association, 6(11), pp. 2103-2109, 2022년 11월.</blockquote><blockquote id="6ba2fbca-c434-4d13-ba5c-77ea62fcfb8f" class="">고대준, 황현상, <strong>이건영</strong>, 김영원, 이의철, <strong>&quot;RGB 카메라를 이용한 다중 생체 신호 검출 통합 시스템 설계 및 개발,&quot;</strong> 차세대융합기술학회논문지 제5권 제5호, pp. 749-756, 2021년 10월.</blockquote><h3 id="b4567eae-9241-4243-8254-a273ff37b497" class="">Domestic conference (1건)</h3><blockquote id="91374006-eba4-43f9-8b84-4b555e24d343" class="">이채원, 윤성빈, 조철우, 황현상, <strong>이건영</strong>, 이의철, <strong>&quot;License plate image enhancement based on super-resolution Generative Adversarial Networks,&quot;</strong> 제40회 한국법과학회 추계학술대회, 2021.11.23 ~ 2021.11.30, 온라인.</blockquote><hr id="f93d1848-c397-4e5b-acb0-e7a70ccbcd50"/><h1 id="66d69fc7-f30b-4a7c-90f7-5086c61785cb" class="block-color-default">Patents [<a href="https://pr.smu.ac.kr/property/%EB%93%B1%EB%A1%9D%ED%8A%B9%ED%97%88">pr.smu.ac.kr</a>]</h1><p id="07b31f67-a2ef-41ad-aeb3-9929e01f072f" class=""><strong>[R#]: Patent Registration, [A#]: Patent Application (#: Count)</strong></p><h3 id="ef54cb2e-3a58-4162-a7c9-44756521943a" class="">Patent Registration</h3><blockquote id="2ef2cd0d-0528-4db5-904e-357fefe2581a" class="">[R5] <strong>Lee, Kunyoung</strong>  and Lee, Eui Chul, <strong>“Method and apparatus for remote photoplethysmogram,”</strong> Korean Patent Registration No: 10-2542525-0000 Date: 2023.06.07</blockquote><blockquote id="768281a6-12d6-46a3-98c3-b6ab00b92bf1" class="">[R4] <strong>Lee, Kunyoung</strong>;  Lee, E. C.; Nam, U.; Kim, Y; <strong>“Deivce and method of screening dementia through analysis between face direction and gaze,”</strong> Korean Patent Registration No: 10-2446848-0000 Date: 2022.09.20</blockquote><blockquote id="761c059d-ff59-487e-b374-afa38e86543d" class="">[R3] <strong>Lee, Kunyoung</strong>;  Lee, E. C.; Kim, T.; Nam, U., <strong>“Apparatus and method for measuring psychological anxiety”</strong> Korean Patent Registration No: 10-2235932-0000 Date: 2021.03.30</blockquote><blockquote id="6d413977-dbee-465c-9c25-87fd52f6e9b3" class="">[R2] <strong>Lee, Kunyoung</strong>  and Lee, Eui Chul,, <strong>“Apparatus and method for measuring face symmetry” </strong>Korean Patent Registration No: 10-211515-00000 Date: 2020.05.20</blockquote><blockquote id="79cb76c5-e64c-4dfb-8f8e-2e7311b44412" class="">[R1] <strong>Lee, Kunyoung</strong>; Lee, E. C.; Kim, J. M., Jang, W., Han, J., <strong>“Apparatus and method for measuring heartbeat using triaxial accelerometer”</strong> Korean Patent Registration No: 10-1986213-0000 Date: 2019.05.30</blockquote><h3 id="ace6a29c-a063-45c2-aa87-f8238b69c368" class="block-color-default">Patent Application</h3><blockquote id="3e93bec6-8b1a-45c0-b223-40dea09c9da7" class="">[A7] <strong>&quot;원격 심박수 측정을 위한 심박 신호를 선별하는 방법 및 장치&quot;</strong>, 국내특허출원 (출원번호: 10-2023-0059266, 출원일: 2023.05.08), 발명자(이의철, <strong>이건영</strong>, 김승현, 박신위, 안병선, 서현수)</blockquote><blockquote id="4175c1a1-0f11-4606-ad93-aa0d085ccbb4" class="">[A6] <strong>&quot;다중 출력에 대한 표정 인식을 위한 뉴럴 네트워크 모델의 경량화 장치 및 방법&quot;</strong>, 국내특허출원 (출원번호: 10-2023-0053584, 출원일: 2023.04.24), 발명자(이의철, 김승현, <strong>이건영</strong>)</blockquote><blockquote id="ef5fbc19-e47c-4887-80c3-19581e066324" class="">[A5] <strong>&quot;심박수 산출 장치 및 방법&quot;</strong>, 국내특허출원 (출원번호: 10-2023-0160496, 출원일: 2023.11.20), 발명자(이의철, <strong>이건영</strong>, 김승현, 전용권, 이지은, 이강인)</blockquote><blockquote id="95a7a2d2-37f7-48ae-9317-e94ac683ca19" class="">[A4] <strong>&quot;원격 광용적맥파신호를 이용한 위조 얼굴 판별 방법 및 장치&quot;</strong>, 국내특허출원 (출원번호: 10-2022-0109832, 출원일: 2022.08.31), 발명자(이의철, <strong>이건영</strong>, 유호준, 오재무)</blockquote><blockquote id="7639a428-d00c-4d62-8d06-91d53270e7be" class="">[A3] <strong>&quot;기계 학습 기반 시선 추적 장치 및 방법&quot;</strong>, 국내특허출원 (출원번호: 10-2021-0045737, 출원일: 2021.04.08), 발명자(이의철, <strong>이건영</strong>, 신유진, 한우정), 출원인(상명대학교산학협력단) (비대면)</blockquote><blockquote id="84919c4d-34a5-4b8c-990e-4e5de07d9d75" class="">[A2] <strong>&quot;영상 인식 기반 지폐 일련번호 인식 장치 및 방법&quot;</strong>, 국내특허출원 (출원번호: 10-2021-0016890, 출원일: 2021.02.05), 발명자(이의철, 장우혁, <strong>이건영</strong>, 전수민, 석채린, 신광용, 이덕형, 김수미, 김상오)</blockquote><blockquote id="3802e76e-655e-4ea6-a428-96b15538fc49" class="">[A1] <strong>&quot;얼굴표정 분석기반 자폐 스펙트럼 장애 평가 방법&quot;</strong>, 국내특허출원 (출원번호: 10-2020-0160142, 출원일: 2020.11.25), 발명자(이의철,<strong>이건영</strong>, 김영원)</blockquote>
      
    <hr id="940de8ef-f24a-47ed-ad6e-1193659c0422"/><h1 id="90ede0fc-254a-4c9b-9632-ce2b2ac353a1" class=""><strong>Research Projects</strong></h1><h2 id="ded83e5a-ae10-44f2-90d0-8ef438195780" class="">Grant (9건)</h2><blockquote id="b1c5cca3-e194-4338-ba34-dcd1524a190f" class=""><strong>[한국연구재단(NRF)] 비접촉 생체신호 추출 및 생체정보 융합을 통한 이상징후 판별 기술 개발, </strong> <em>Jul. 2022 ~ Present<br/></em><strong>[과학기술정보통신부(MSIT)] 비접촉 생체신호 기반 운동효과 피드백 기능을 갖춘 AI 홈 트레이닝 시스템 제작,</strong>  <em>Apr. 2022 ~ Feb. 2023<br/></em><strong>[과학기술일자리진흥원(COMPA)] 머신러닝 화재 판정 모듈 및 AI학습플랫폼 고도화.안정화 개발, [Project Manager],  </strong><em>Oct. </em>2021 ~ <em>Mar. </em>2023<br/><strong>[산업통상자원부(KIAT)] 안면인식 위변조 보안을 적용한 무인매장 솔루션 연구개발 및 실증, </strong> <em>Sep. 2021 ~ Aug. 2022<br/></em><strong>[한국데이터산업진흥원(Kdata)] 2021 데이터바우처 지원 사업(수요기관:기산전자),</strong> <strong> </strong><em>Jul. 2021 ~ Nov. 2021<br/></em><strong>[한국전자기술연구원(KETI)] 상태데이터 통합 인터페이스 제작,</strong>  <em>Oct. 2020 ~ Dec. 2020<br/></em><strong>[한국연구재단(NRF)] 카메라 기반 통합형 자율신경 반응 측정 모델 연구,</strong>  <em>Jun. 2019 ~ Feb. 2022<br/></em><strong>[한국연구재단(NRF)] 생체기반 영상정보 정량적 분석 시스템,</strong>  <em>Mar. 2017 ~ Feb. 2021<br/></em><strong>[산업통상자원부(MOTIE)] 마음-몸 피드백을 통한 감정 치유를 위한 비접촉식 센싱 기반 인간 내면상태 인식 및 미러링 표출 상호작용 로봇 기술 개발,  </strong><em>Mar. 2017 ~ Aug. 2021</em></blockquote><h2 id="b478982e-fc4f-4cca-8e12-753972b3c6aa" class="">Contract (10건)</h2><blockquote id="27b78492-10f3-4725-b428-03dd15919b81" class=""><strong>[HYUNDAI NGV] 비전 기반 심박수 측정 시스템 개발 및 고도화, [Project Manager],  </strong><em>Jul. 2023 ~ Oct. 2023<br/></em><strong>[HYUNDAI NGV] 운전자 감성인식을 위한 비접촉 생체신호 측정기술 개발 [Project Manager],  </strong><em>Oct. 2022 ~ Sep. 2023<br/></em><strong>[이후시스(주)] 비접촉 생체반응 측정 기술 개발, [Project Manager],</strong>  <em>Aug. 2022 ~ Jan. 2023<br/></em><strong>[SL Corporation] 얼굴 표정 및 HRV 특징을 이용한 운전자 감정인식 알고리즘 개발, [Project Manager],</strong>  <em>Jun. 2022 ~ Dec. 2023<br/></em><strong>[(주)이모코그] 신경퇴행성 질환자의 생체신호 수집 및 분석 기술 개발,</strong>  <em>Mar. 2022 ~ Aug. 2022<br/></em><strong>[HYUNDAI NGV] 비전기반 생체반응 및 행동상태 측정 연구,  [Project Manager],  </strong>Dec. 2021 ~ Dec. 2022<br/><strong>[주식회사 애니랙티브] RGB 카메라 기반 생리반응 측정 및 인터렉션 기술개발, [Project Manager],</strong>  <em>Aug. 2021 ~ Oct. 2021<br/></em><strong>[KETI] 상태데이터 통합 인터페이스 고도화 용역,</strong>  <em>Jun. 2021 ~ Nov. 2021<br/></em><strong>[대검찰청] AI를 이용한 차량번호 인식 기법 연구, [Project Manager],</strong>  <em>Mar. 2021 ~ Dec. 2021<br/></em><strong>[기산전자] Deep Learning 기반 PC OCR 엔진 기술 개발, [Project Manager],  </strong><em>Mar. 2020 ~ Jun. 2020</em></blockquote><hr id="0b21e1e9-777e-4a67-ab43-16a5cf550402"/>
    
    <h1 id="5222fc28-9b1d-4283-84cb-f018a9dc81f4" class="">Demo (youtube)</h1>
    <h3 id="c0ee80ca-1517-45f0-962d-b690d44da8c1" class="">Remote Photoplethysmography</h3>
    <div id="youtube_column" class="column-list"><div id="rppg_video" style="width:49%" class="column">
    <div style="overflow:hidden;position: relative;"><iframe frameborder="0" scrolling="no" marginheight="0" marginwidth="0"width="540" height="360" type="text/html" src="https://www.youtube.com/embed/n_agiICg5PQ?autoplay=0&fs=0&iv_load_policy=3&showinfo=0&rel=0&cc_load_policy=0&start=0&end=0"></iframe><div style="position: absolute;bottom: 10px;left: 0;right: 0;margin-left: auto;margin-right: auto;color: #000;text-align: center;"><div style="overflow: auto; position: absolute; height: 0pt; width: 0pt;"></div><script type="text/javascript" src="https://www.embedista.com/j/ytvideo.js"></script></div><style>.newst{position:relative;text-align:right;height:420px;width:520px;} #gmap_canvas img{max-width:none!important;background:none!important}</style></div></div>
    <div id="rppg_video" style="width:49%" class="column">
    <div style="overflow:hidden;position: relative;"><iframe frameborder="0" scrolling="no" marginheight="0" marginwidth="0"width="540" height="360" type="text/html" src="https://www.youtube.com/embed/Y4lsUkucfks?autoplay=0&fs=0&iv_load_policy=3&showinfo=0&rel=0&cc_load_policy=0&start=0&end=0"></iframe><div style="position: absolute;bottom: 10px;left: 0;right: 0;margin-left: auto;margin-right: auto;color: #000;text-align: center;"><div style="overflow: auto; position: absolute; height: 0pt; width: 0pt;">Generated by <a href="https://www.embedista.com/embed-youtube-video">Embed Youtube Video</a> online</div><script type="text/javascript" src="https://www.embedista.com/j/ytvideo.js"></script></div><style>.newst{position:relative;text-align:right;height:420px;width:520px;} #gmap_canvas img{max-width:none!important;background:none!important}</style></div></div></div>

    </p><h3 id="c1e0c9eb-40bc-461f-b940-299df7653d9f" class="">Gaze Analysis</h3>
    <div id="gaze_column" class="column-list">
    <div id="gaze_video2" style="width:49%" class="column">
    <div style="overflow:hidden;position: relative;"><iframe frameborder="0" scrolling="no" marginheight="0" marginwidth="0"width="540" height="360" type="text/html" src="https://www.youtube.com/embed/1uOyHijAFpQ?autoplay=0&fs=0&iv_load_policy=3&showinfo=0&rel=0&cc_load_policy=0&start=0&end=0"></iframe><div style="position: absolute;bottom: 10px;left: 0;right: 0;margin-left: auto;margin-right: auto;color: #000;text-align: center;"><div style="overflow: auto; position: absolute; height: 0pt; width: 0pt;">Generated by <a href="https://www.embedista.com/embed-youtube-video">Embed Youtube Video</a> online</div><script type="text/javascript" src="https://www.embedista.com/j/ytvideo.js"></script></div><style>.newst{position:relative;text-align:right;height:420px;width:520px;} #gmap_canvas img{max-width:none!important;background:none!important}</style></div></div>
    <div id="gaze_video1" style="width:49%" class="column">
    <div style="overflow:hidden;position: relative;"><iframe frameborder="0" scrolling="no" marginheight="0" marginwidth="0"width="540" height="360" type="text/html" src="https://www.youtube.com/embed/cMd2WTIpTWA?autoplay=0&fs=0&iv_load_policy=3&showinfo=0&rel=0&cc_load_policy=0&start=0&end=0"></iframe><div style="position: absolute;bottom: 10px;left: 0;right: 0;margin-left: auto;margin-right: auto;color: #000;text-align: center;"><div style="overflow: auto; position: absolute; height: 0pt; width: 0pt;">Generated by <a href="https://www.embedista.com/embed-youtube-video">Embed Youtube Video</a> online</div><script type="text/javascript" src="https://www.embedista.com/j/ytvideo.js"></script></div><style>.newst{position:relative;text-align:right;height:420px;width:520px;} #gmap_canvas img{max-width:none!important;background:none!important}</style></div></div>
    </p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body>
    </div>
    </li>
